import trino
import sys
import argparse
import os
import concurrent.futures
import time

def get_connection(host, username, password):
    return trino.dbapi.connect(
        host=host,
        port=443,
        user=username,
        http_scheme="https",
        auth=trino.auth.BasicAuthentication(username, password),
        verify=False,
    )

def catalog_exists(conn, catalog_name):
    cursor = conn.cursor()
    cursor.execute("SHOW CATALOGS")
    catalogs = [row[0] for row in cursor.fetchall()]
    return catalog_name in catalogs

def schema_exists(conn, catalog_name, schema_name):
    cursor = conn.cursor()
    cursor.execute(f"SHOW SCHEMAS FROM {catalog_name}")
    schemas = [row[0] for row in cursor.fetchall()]
    return schema_name in schemas

def create_schema(conn_source, conn_dest, source_catalog, dest_catalog, schema_name, dry_run):
    cursor_source = conn_source.cursor()
    cursor_source.execute(f"SHOW CREATE SCHEMA {source_catalog}.{schema_name}")
    create_schema_sql = cursor_source.fetchone()[0]

    if dry_run:
        print(f"[Dry Run] Would execute schema creation: {create_schema_sql}")
    else:
        cursor_dest = conn_dest.cursor()
        cursor_dest.execute(create_schema_sql)
        print(f"Schema {schema_name} created in {dest_catalog}.")

def get_all_tables(conn_source, catalog_name, schema_name):
    cursor = conn_source.cursor()
    cursor.execute(f"SHOW TABLES FROM {catalog_name}.{schema_name}")
    return [row[0] for row in cursor.fetchall()]

def create_table(conn_source, conn_dest, source_catalog, dest_catalog, schema_name, table_name, dry_run):
    cursor_source = conn_source.cursor()
    cursor_source.execute(f"SHOW CREATE TABLE {source_catalog}.{schema_name}.{table_name}")
    create_table_sql = cursor_source.fetchone()[0]

    if dry_run:
        print(f"[Dry Run] Would execute table creation for {table_name}.")
    else:
        cursor_dest = conn_dest.cursor()
        cursor_dest.execute(create_table_sql)
        print(f"Table {table_name} created in {dest_catalog}.{schema_name}.")

def table_exists(conn_dest, catalog_name, schema_name, table_name):
    cursor = conn_dest.cursor()
    cursor.execute(f"SHOW TABLES FROM {catalog_name}.{schema_name}")
    tables = [row[0] for row in cursor.fetchall()]
    return table_name in tables

def main():
    parser = argparse.ArgumentParser(description="Replicate Trino tables between catalogs.")
    parser.add_argument("--source_host", required=True)
    parser.add_argument("--source_catalog", required=True)
    parser.add_argument("--dest_host", required=True)
    parser.add_argument("--dest_catalog", required=True)
    parser.add_argument("--schema_name", required=True)
    parser.add_argument("--full_schema_replicate", required=True, choices=["yes", "no"])
    parser.add_argument("--artifact_path", required=False)
    parser.add_argument("--username", required=True)
    parser.add_argument("--password", required=True)
    parser.add_argument("--dry_run", action="store_true", help="Run without making changes")

    args = parser.parse_args()

    # Connect to source and destination
    conn_source = get_connection(args.source_host, args.username, args.password)
    conn_dest = get_connection(args.dest_host, args.username, args.password)

    # Step 1: Check catalogs
    if not catalog_exists(conn_source, args.source_catalog):
        print(f"Source catalog {args.source_catalog} does not exist!")
        sys.exit(1)
    if not catalog_exists(conn_dest, args.dest_catalog):
        print(f"Destination catalog {args.dest_catalog} does not exist!")
        sys.exit(1)

    print(f"Both source and destination catalogs verified.")

    # Step 2: Check schema
    if not schema_exists(conn_dest, args.dest_catalog, args.schema_name):
        print(f"Schema {args.schema_name} does not exist in destination. Creating...")
        create_schema(conn_source, conn_dest, args.source_catalog, args.dest_catalog, args.schema_name, args.dry_run)
    else:
        print(f"Schema {args.schema_name} already exists in destination.")

    # Step 3: Decide tables
    if args.full_schema_replicate == "yes":
        tables_to_replicate = get_all_tables(conn_source, args.source_catalog, args.schema_name)
    else:
        if not args.artifact_path:
            print("Artifact path required for partial replication.")
            sys.exit(1)
        with open(args.artifact_path, "r") as f:
            tables_to_replicate = [t.strip() for t in f.read().split(",")]

    total_tables = len(tables_to_replicate)
    print(f"Total tables to replicate: {total_tables}")

    # Step 4: Replication with parallelism
    def replicate(table_name):
        if not table_exists(conn_dest, args.dest_catalog, args.schema_name, table_name):
            create_table(conn_source, conn_dest, args.source_catalog, args.dest_catalog, args.schema_name, table_name, args.dry_run)
        else:
            print(f"Table {table_name} already exists. Skipping.")

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        futures = [executor.submit(replicate, table) for table in tables_to_replicate]
        for i, f in enumerate(concurrent.futures.as_completed(futures), 1):
            print(f"[{i}/{total_tables}] Table processed.")

if __name__ == "__main__":
    main()
